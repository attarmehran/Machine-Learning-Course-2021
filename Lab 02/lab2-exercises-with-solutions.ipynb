{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "identity": "6321-2021-fall-lab2-solution"
   },
   "source": [
    "# Lab 2 Exercises for COMP 6321 Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you'll translate mathematics from lecture into practical Numpy code. Specifically, you'll implement _linear least squares regression_ and _logistic regression_ \"from scratch\" and compare the results of your own implementations to those of *scikit-learn*, a popular machine learning package.\n",
    "\n",
    "<span style=\"color:red\"><i>Warning.</i></span> Many of the code cells in this notebook re-use the variable names like `X` or `y`, but assign them different data. If you run cells out of order, you may get unexpected results or errors, so be careful when switching between exercises.\n",
    "\n",
    "**Run the code cell below** to import the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lab2 requires a good understanding of Numpy and Matplotlib. Please complete Lab1 before attempting Lab2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "<div style=\"border-bottom: 3px solid black\"></div>\n",
    "\n",
    "## 1. Plotting a 2D function and its gradient\n",
    "\n",
    "Exercises 1.1&ndash;1.4 ask you to plot a function and its gradient, then optimize it with gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 1.1 &mdash; Evaluate a function on a 2D grid\n",
    "\n",
    "The Python function below takes another function, *func*, as an argument, and evaluates it on a 2D grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_grid_unvectorized(func, extent, numsteps):\n",
    "    \"\"\"\n",
    "    Evaluates func(x1, x2) for each combination in a 2D grid.\n",
    "      func: callable - function to evaluate for each grid element\n",
    "      extent: tuple - grid extent as (x1min, x1max, x2min, x2max)\n",
    "      numsteps: int - number of grid steps (same for each dimension)\n",
    "    \"\"\"\n",
    "    x1min, x1max, x2min, x2max = extent\n",
    "    x1 = np.empty((numsteps, numsteps))\n",
    "    x2 = np.empty((numsteps, numsteps))\n",
    "    y  = np.empty((numsteps, numsteps))\n",
    "    for i in range(numsteps):\n",
    "        for j in range(numsteps):\n",
    "            x1[i,j] = x1min + j*(x1max-x1min)/(numsteps-1)\n",
    "            x2[i,j] = x2min + i*(x2max-x2min)/(numsteps-1)\n",
    "            y[i,j] = func(x1[i,j], x2[i,j])\n",
    "    return x1, x2, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Run the code cell below** to see an example of its output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2, y = eval_on_grid_unvectorized(lambda x1, x2: x1 + x2, (-1, 1, 0, 2), 3)\n",
    "print(\"x1:\"); print(x1)\n",
    "print(\"x2:\"); print(x2)\n",
    "print(\"y:\");  print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write a vectorized version of *eval_on_grid*** in the code cell below. Your code should be fully vectorized, with no for-loops. Consider using [*np.meshgrid*](https://numpy.org/doc/stable/reference/generated/numpy.meshgrid.html) and [*np.linspace*](https://numpy.org/doc/stable/reference/generated/numpy.linspace.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_on_grid_vectorized(func, extent, numsteps):\n",
    "    # Your code here. Aim for 4-6 lines.\n",
    "    x1min, x1max, x2min, x2max = extent\n",
    "    x1steps = np.linspace(x1min, x1max, numsteps)\n",
    "    x2steps = np.linspace(x2min, x2max, numsteps)\n",
    "    x1, x2 = np.meshgrid(x1steps, x2steps)\n",
    "    y = func(x1, x2)\n",
    "    return x1, x2, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your answer** by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = (lambda x1, x2: x1 * x2, (-1, 1, -4, 4), 20)\n",
    "r1 = eval_on_grid_unvectorized(*args)       # r1 = (x1, x2, y) for unvec version\n",
    "r2 = eval_on_grid_vectorized(*args)         # r2 = (x1, x2, y) for vec version\n",
    "for v1, v2 in zip(r1, r2):\n",
    "    np.testing.assert_almost_equal(v1, v2)  # check that x1, x2, or y matches\n",
    "print(\"Correct!\")\n",
    "\n",
    "import timeit\n",
    "args = (lambda x1, x2: x1**2 + 0.5*x2, (0, 1, 0, 1), 200)\n",
    "unvec_time = timeit.timeit('eval_on_grid_unvectorized(*args)', setup=\"from __main__ import eval_on_grid_unvectorized, args\", number=10)\n",
    "vec_time   = timeit.timeit('eval_on_grid_vectorized(*args)',   setup=\"from __main__ import eval_on_grid_vectorized, args\", number=10)\n",
    "print(\"Your vectorized code ran %.1fx faster than the original code on a 200x200 grid\" % (unvec_time/vec_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 1.2 &mdash; Plot a function as a heatmap\n",
    "\n",
    "Consider the function\n",
    "$$\n",
    "f(x_1, x_2) = (\\textstyle \\frac{1}{2}x_1 + x_2 + 1)^2 + (x_2 - 1)^2\n",
    "$$\n",
    "\n",
    "**Write code to compute $f(x_1, x_2)$**. Your code should run for $x_1$ and $x_2$ either numbers or Numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x1, x2):\n",
    "    # Your code here. Aim for 1-3 lines.\n",
    "    return (.5*x1 + x2 + 1)**2 + (x2 - 1)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your answer** by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = f(2.5, -4.0)\n",
    "assert isinstance(v, float), \"Expected float args to give float result\"\n",
    "assert v == 28.0625, \"Wrong return value for float\"\n",
    "v = f(np.eye(3, 4), np.arange(12).reshape(3, 4))\n",
    "assert isinstance(v, np.ndarray), \"Expected ndarray args to give ndarray result\"\n",
    "np.testing.assert_equal(v, [[3.25, 4., 10., 20.], [34., 58.25, 74., 100.], [130., 164., 213.25, 244.]])\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write plotting code** to visualize your $f(x_1, x_2)$ function over the interval $x_1 \\in [-6, 6]$ and $x_2 \\in [-3, 3]$. Your plot should look like this:\n",
    "\n",
    "<center><img style=\"display:inline\" src=\"img/fig-exercise12-heatmap.png\"/></center>\n",
    "\n",
    "Specifically:\n",
    "\n",
    "* Use your *eval_on_grid_vectorized* to compute all the grid positions and function values at sufficient resolution.\n",
    "* Use [*plt.figure*](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.figure.html) with a *figsize* designed to make the plot twice as wide as it is tall\n",
    "* Use [*plt.imshow*](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.imshow.html) and specify *origin* and *extent* to ensure the function values are plotted at the right positions.\n",
    "* Use [*plt.colorbar*](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.colorbar.html) and specify *fraction=0.046/2* to make its height match the main figure\n",
    "* Use [*plt.contour*](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.contour.html) and use [*np.logspace*](https://numpy.org/doc/stable/reference/generated/numpy.logspace.html) to plot 5 contours logarithmically spaced between $0.1$ and $10$ inclusive.\n",
    "* Configure the axis labels and title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_exercise12():\n",
    "    # Your code here. Aim for 9-12 lines.\n",
    "    # Compute grid of x1, x2 values, and compute y = f(x1, x2) for each\n",
    "    extent = (-6, 6, -3, 3)\n",
    "    x1, x2, y = eval_on_grid_vectorized(f, extent, 100)\n",
    "            \n",
    "    # Plot y value at each (x1, x2)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(y, origin='lower', extent=extent)\n",
    "    plt.colorbar(fraction=0.046/2)\n",
    "    plt.contour(x1, x2, y, levels=np.logspace(-1, 1, 5), colors='white', linestyles=':')\n",
    "\n",
    "    # Configure plot\n",
    "    plt.xlabel('$x_1$')  # The $...$ tells Matplotlib to go into LaTeX mode, where\n",
    "    plt.ylabel('$x_2$')  # subscripts (x_1) and superscripts (x^2) are recognized\n",
    "    plt.title('Plot of $f(x_1, x_2)$')\n",
    "\n",
    "plot_exercise12()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 1.3 &mdash; Plot gradients as a vector field\n",
    "\n",
    "The gradient of $f(x_1, x_2)$ is a vector-valued function:\n",
    "$$\n",
    "\\nabla f(x_1, x_2) = \\begin{bmatrix}\n",
    "\\frac{\\partial f}{\\partial x_1}(x_1, x_2) \\\\\n",
    "\\frac{\\partial f}{\\partial x_2}(x_1, x_2) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Write code to compute $\\nabla f(x_1, x_2)$**. You'll need to use basic calculus (differentiation) to figure out the correct formulas to implement, by yourself. Consider using [*np.stack*](https://numpy.org/doc/stable/reference/generated/numpy.stack.html) to form the final array of gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_grad(x1, x2):\n",
    "    \"\"\"\n",
    "    Returns an ndarray 'grad' where grad[0,...] and grad[1,...] are the 1st and \n",
    "    2nd gradient components (respectively) when evaluated at x1[...] and x2[...].\n",
    "    In other words, if x1 and x2 have shape (...) then grad has shape (2,...).\n",
    "    \"\"\"\n",
    "    # Your code here. Aim for 1-3 lines.\n",
    "    return np.stack((.5*x1 + x2 + 1, x1 + 4*x2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your answer** by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "v = f_grad(3.5, -15.0)\n",
    "assert isinstance(v, np.ndarray), \"Expected float args to give ndarray result\"\n",
    "np.testing.assert_equal(v, [-12.25, -56.5])\n",
    "v = f_grad(np.eye(3, 4), np.arange(12).reshape(3, 4))\n",
    "assert isinstance(v, np.ndarray), \"Expected array args to give ndarray result\"\n",
    "assert v.shape == (2, 3, 4), \"Result was wrong shape\"\n",
    "np.testing.assert_equal(v, [[[ 1.5,  2. ,  3. ,  4. ], [ 5. ,  6.5,  7. ,  8. ], [ 9. , 10. , 11.5, 12. ]],\n",
    "                            [[ 1. ,  4. ,  8. , 12. ], [16. , 21. , 24. , 28. ], [32. , 36. , 41. , 44. ]]])\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write plotting code** to overlay the gradient $\\nabla f(x_1, x_2)$ over your figure from Exercise 1.2. Your plot should look like this:\n",
    "\n",
    "<center><img style=\"display:inline\" src=\"img/fig-exercise12-vecfield.png\"/></center>\n",
    "\n",
    "Specifically:\n",
    "\n",
    "* Use your *eval_on_grid_vectorized* to compute all the grid positions and gradient values at suitable resolution.\n",
    "* Use [*plt.quiver*](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.quiver.html) to plot the vector field of gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_exercise13():\n",
    "    plot_exercise12()  # Start with your previous plot\n",
    "\n",
    "    # Your code here. Aim for 2-4 lines.\n",
    "    extent = (-6, 6, -3, 3)\n",
    "    x1, x2, grad = eval_on_grid_vectorized(f_grad, extent, 25)\n",
    "    plt.quiver(x1, x2, *grad, color='white')\n",
    "    \n",
    "plot_exercise13()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 1.4 &mdash; Gradient descent on a function\n",
    "\n",
    "Gradient descent is an iterative algorithm that repeatedly takes steps in the direction opposite the gradient:\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_\\text{new} = \\mathbf{x}_\\text{old} - \\eta \\nabla f(\\mathbf{x}_\\text{old})\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x} = (x_1, x_2)$. The step size is scaled by the *learning rate*, which is chosen to be some constant $\\eta \\gt 0$.\n",
    "\n",
    "**Write a function** that runs a specific number of steps of gradient descent on the function $f(x_1, x_2)$ from Exercise 1.2. To do this, use the *f_grad* function that you wrote for Exercise 1.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_on_f(x_init, learn_rate, num_steps):\n",
    "    \"\"\"\n",
    "    Runs num_steps of gradient descent from point x_init using\n",
    "    the given learning rate, and returns the new x coordinate.\n",
    "    Here x_init is an ndarray with shape (2,).\n",
    "    \"\"\"\n",
    "    # Your code here. Aim for 4-5 lines.\n",
    "    x = x_init\n",
    "    for i in range(num_steps):\n",
    "        x = x - learn_rate*f_grad(*x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your answer** by running the code cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = gradient_descent_on_f(np.array([-4, 1]), 100.0, 1)\n",
    "assert isinstance(x, np.ndarray), \"Expected ndarray\"\n",
    "assert np.array_equal(x, [-4, 1]), \"Gradient descent shouldn't move away from optimal value!\"\n",
    "x = gradient_descent_on_f(np.array([2, 0]), 0.25, 1)\n",
    "assert np.array_equal(x, [1.5, -0.5]), \"The first gradient step seems to be wrong!\"\n",
    "x = gradient_descent_on_f(x, 0.1, 3)\n",
    "assert np.allclose(x, [1.1294375, -0.369625]), \"The gradient seems to be wrong after a few steps!\"\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot the path of gradient descent** by running the code cell below. You should see a path of little red 'x' marks that converge near $(x_1^*, x_2^*)=(-4,1)$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_exercise13()  # Plot the vector field first\n",
    "\n",
    "# Run gradient descent in chunks of 5 steps at a time, plotting the resulting 'x' after each chunk\n",
    "learn_rate = 0.05\n",
    "x_init = np.array([2.0, 1.0])                               # Start from initial point (-2, -1)\n",
    "for num_steps in range(0, 250, 5):                            # Repeatedly run gradient descent from the initial point,\n",
    "    x = gradient_descent_on_f(x_init, learn_rate, num_steps)  # eventually running it for 100 steps.\n",
    "    plt.plot(*x, 'xr');                                    # Add a little 'x' to the plot to show how far it got."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Optional:* Advanced students can try adding \"momentum\" to their implementation of *gradient_descent_on_f*, and then see how it effects the path of optimization. Relevant formulas and helpful visualizations regarding momentum can be found for example in [*Why Momentum Really Works*](https://distill.pub/2017/momentum/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "<div style=\"border-bottom: 3px solid black\"></div>\n",
    "\n",
    "## 2. Linear least squares regression\n",
    "\n",
    "Exercises 2.1&ndash;2.5 ask you to implement linear least squares regression, and to compare your results to applying the scikit-learn **[LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)** model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 2.1 &mdash; Vectorized code for generating predictions from a basic linear model\n",
    "\n",
    "Recall from Lecture 1 that a basic linear model has the form:\n",
    "$$\n",
    "\\hat{y}(\\mathbf{x}, \\mathbf{w}) = \\mathbf{x}^T \\mathbf{w}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{x} &= \\begin{bmatrix} 1 & x_1 & \\ldots & x_D \\end{bmatrix}^T\\\\\n",
    "\\mathbf{w} &= \\begin{bmatrix} w_0 & w_1 & \\ldots & w_D \\end{bmatrix}^T\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "If both $\\mathbf{x}$ and $\\mathbf{w}$ are column vectors, the following Python function would evaluate the linear model $\\hat{y}(\\mathbf{x}, \\mathbf{w})$ correctly:\n",
    "```python\n",
    "def linear_model_predict(x, w):\n",
    "    \"\"\"Returns a prediction from linear model y(x, w) at point x using parameters w.\"\"\"\n",
    "    return x.T @ w   # Return the inner product (dot product) of vectors x and w\n",
    "```\n",
    "However, we want a version of *linear_model_predict* that vectorizes across many $\\mathbf{x}$ simultaneously. Specifically, given a matrix of inputs:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "\\mathbf{x}_1^T\\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{x}_N^T\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "we want *linear_model_predict* to compute a vector of outputs:\n",
    "\n",
    "$$\n",
    "\\hat{\\mathbf{y}} = \\begin{bmatrix}\n",
    "\\mathbf{x}_1^T \\mathbf{w}\\\\\n",
    "\\vdots\\\\\n",
    "\\mathbf{x}_N^T \\mathbf{w}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "However, if we substitute $x$ with $X$ we can no longer use expression `X.T @ w`; the matrix $X^T \\in \\mathbb{R}^{(D+1) \\times N}$ isn't even the right shape to be on the left-hand side of the product. Writing vectorized code is full of annoying little problems like this.\n",
    "\n",
    "**Write a function** that evaluates the linear model in vectorized fashion. Specifically, when given a matrix $X \\in \\mathbb{R}^{N \\times (D+1)}$ as an argument, you should figure out what mathematical expression would result in the $\\hat{\\mathbf{y}}\\in\\mathbb{R}^N$ vector shown above. Hint: the solution is only a small change from `X.T @ w`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model_predict(X, w):\n",
    "    \"\"\"\n",
    "    Returns predictions from linear model y(X, w) at each point X[i,:] using parameters w.\n",
    "    Given X with shape (N,D+1), w must have shape (D+1,) and the result will have shape (N,).\n",
    "    \"\"\"\n",
    "    # Your code here. Aim for 1 line.\n",
    "    return X @ w      # This works for w of 1-dimensional shape (D+1,) or of column-vector shape (D+1,1)\n",
    "    #return w @ X.T  # This also works, but will fail if w is a column vector of shape (D+1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your answer** by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([2, 0.5])                            # Parameters corresponding to the 1D line y = 2 + 0.5*x1\n",
    "X = np.array([[1., -3.], [1.,  3.], [1.,  5.]])   # Evaluate at x1 = -3, 2, 5\n",
    "y = linear_model_predict(X, w)                    # Predict y for all X using w\n",
    "assert isinstance(y, np.ndarray), \"Expected an ndarray!\"\n",
    "assert np.array_equal(y, [0.5, 3.5, 4.5]), \"Wrong predictions!\\n%s\" % y\n",
    "try:\n",
    "    y = linear_model_predict(X, w.reshape(-1, 1))\n",
    "except ValueError:\n",
    "    raise AssertionError(\"Your answer works when 'w' is 1-dimensional, but not when it is a column vector. Try again.\")\n",
    "w = np.array([1, 0.5, 0.25])                      # Parameters corresponding to the 2D plane y = 1 + 0.5*x1 + 0.25*x2\n",
    "X = np.array([[1., -3., 1.], [1.,  3., 0.], [1.,  5., -2.]])   # Evaluate at different (x1, x2) points\n",
    "y = linear_model_predict(X, w)                    # Predict y for all X using w\n",
    "assert np.array_equal(y, [-0.25, 2.5, 3.0]), \"Wrong predictions for 2-dimensional feature space!\\n%s\" % y\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot several predictions at once** by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([2, 0.5])           # Parameters corresponding to the 1D line y = 2 + 0.5*x1\n",
    "x0 = np.ones(20)                 # A column of 1s so that the bias term w[0] gets added\n",
    "x1 = np.linspace(-5, 5, 20)      # A column of x values ranging from [-5, 5]\n",
    "X = np.column_stack([x0, x1])    # A 20x2 matrix where X[i,:] is the ith x vector\n",
    "y = linear_model_predict(X, w)   # Evaluate all x values\n",
    "plt.scatter(x1, y, 10, 'r')\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.title(\"Sample predictions for linear model $y=2 + \\\\frac{1}{2}x_1$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 2.2 &mdash; Linear least squares regression by gradient descent\n",
    "\n",
    "Here you'll implement a 'learning' algorithm for linear least squares regression. Recall from Lecture 1 that the least squares training objective is:\n",
    "\n",
    "$$\n",
    "\\ell(\\mathbf{w}) = \\frac{1}{2} \\sum_{i=1}^N (y - \\hat{y}(\\mathbf{x}_i, \\mathbf{w}))^2\n",
    "$$\n",
    "\n",
    "The gradient for the above training objective is on the slide titled \"Linear least squares *learning*\" from Lecture 1. You'll need it.\n",
    "\n",
    "**Write a function** to implement linear least squares regression by gradient descent. Use the `@` operator (matrix multiplication) in your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_by_gradient_descent(X, y, w_init, learn_rate=0.05, num_steps=500):\n",
    "    \"\"\"\n",
    "    Fits a linear model by gradient descent.\n",
    "    \n",
    "    If the feature matrix X has shape (N,D) the targets y should have shape (N,)\n",
    "    and the initial parameters w_init should have shape (D,).\n",
    "    \n",
    "    Returns a new parameter vector w that minimizes the squared error to the targets.\n",
    "    \"\"\"\n",
    "    # Your code here. Aim for 4-5 lines.\n",
    "    w = w_init\n",
    "    for i in range(num_steps):\n",
    "        grad = (X.T @ X) @ w - X.T @ y\n",
    "        w = w - learn_rate*grad\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your answer** by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, 0.0], [1, 1.0], [1, 2.0]])\n",
    "y = np.array([4.0, 3.0, 2.0])\n",
    "w = linear_regression_by_gradient_descent(X, y, np.array([0.0, 0.0]))\n",
    "assert isinstance(w, np.ndarray), \"Expected ndarray!\"\n",
    "assert w.shape == (2,), \"Wrong shape for final parameters!\\n%s\" % w\n",
    "assert np.allclose(w, [4, -1]), \"Wrong values for final parameters!\\n%s\" % w\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 2.3 &mdash; Linear least squares regression by direct solution\n",
    "\n",
    "As discussed in class, the optimal parameters $\\mathbf{w}^*$ for linear least squares regression can be solved *directly*, rather than iteratively.\n",
    "\n",
    "**Write a function** to solve linear least squares regression directly. Use Numpy's matrix inverse function **[np.linalg.inv](https://numpy.org/devdocs/reference/generated/numpy.linalg.inv.html)** in your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression_by_direct_solve(X, y):\n",
    "    \"\"\"Fits a linear model by directly solving for the optimal parameter vector w.\"\"\"\n",
    "    # Your code here. Aim for 1-2 lines.\n",
    "    return np.linalg.inv(X.T @ X) @ X.T @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = linear_regression_by_direct_solve(X, y)\n",
    "assert isinstance(w, np.ndarray), \"Expected ndarray!\"\n",
    "assert w.shape == (2,), \"Wrong shape for final parameters!\\n%s\" % w\n",
    "assert np.allclose(w, [4, -1]), \"Wrong values for final parameters!\\n%s\" % w\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 2.4 &mdash; Run linear least squares regression and plot the result\n",
    "\n",
    "For this exercise you'll need to define Numpy arrays that correspond to the following training data:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "1 & -2.2\\\\\n",
    "1 & -0.3\\\\\n",
    "1 &  1.5\\\\\n",
    "1 &  4.8\n",
    "\\end{bmatrix}, \\quad\n",
    "\\mathbf{y} = \\begin{bmatrix}\n",
    "-1.2 \\\\\n",
    "1.5\\\\\n",
    "4.2\\\\\n",
    "5.3\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Write code** to create the following plot:\n",
    "![image](img/fig-linear-regression-1d-train-and-test.png)\n",
    "\n",
    "Your code should follow this sequence of steps:\n",
    "1. Make ndarrays $X$ and $\\mathbf{y}$ that contain the above training set.\n",
    "2. Plot the training set in blue. Use the $x$ coordinates from the second column of $X$, ignoring the first column.\n",
    "3. Run linear least squares regression on $(X, \\mathbf{y})$ to get fitted parameters $\\mathbf{w}$; use your *linear_regression_by_direct_solve* function.\n",
    "4. Define a \"test set\" of 20 equally-spaced values of $x$ in range $[-5, 5]$. You will need to build a new matrix $X_\\text{test}$ with $1$ in the first column and the 20 distinct $x$ values in the second column. See how this is done in the last code cell of Exercise 2.1.\n",
    "5. Predict 20 $y$ values corresponding to the 20 rows of $X_\\text{test}$ by applying a linear model with your fitted parameters $\\mathbf{w}$. Do this with single call to your *linear_model_predict* function. \n",
    "6. Plot the predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 1. Define the training set. Aim for 2 lines.\n",
    "X = np.array([[1, -2.2],\n",
    "              [1, -0.3],\n",
    "              [1,  1.5],\n",
    "              [1,  4.8]])\n",
    "y = np.array([-1.2, 1.5, 4.2, 5.3])\n",
    "\n",
    "# 2. Plot the training set. Aim for 1 line.\n",
    "plt.scatter(X[:,1], y, edgecolors='b', facecolors='none', label='training points')\n",
    "\n",
    "# 3. Run linear least squares regression on the training set to compute 'w'. Aim for 1 line.\n",
    "w = linear_regression_by_direct_solve(X, y)\n",
    "\n",
    "# 4. Define the test set matrix of shape (20,2). Aim for 1-3 lines.\n",
    "X_test = np.column_stack([np.ones(20), np.linspace(-5, 5, 20)])\n",
    "\n",
    "# 5. Use the linear model to make predictions on the test set. Aim for 1 line.\n",
    "y_test = linear_model_predict(X_test, w)\n",
    "\n",
    "# 6. Plot the test predictions. Aim for 1 line, plus a few lines to configure the plot (axis labels etc).\n",
    "plt.scatter(x1, y_test, 10, 'r', label='test predictions')\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.title(\"linear least squares regression\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 2.5 &mdash; Run scikit-learn LinearRegression\n",
    "\n",
    "The scikit-learn package provides a **[LinearRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)** object to perform linear least squares regression (also known as \"ordinary\" least squares).\n",
    "\n",
    "**Write code to fit a LinearRegression model** using the same training matrix $X$ that you defined as part of Exercise 2.4. There are only two steps:\n",
    "1. Create the _LinearRegression_ object. Use the *fit_intercept=False* option when creating the *LinearRegression* object (see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)), since the $X$ matrix already has a column of 1s corresponding to an intercept parameter (the 'bias' parameter).\n",
    "2. Fit the _LinearRegression_ object to the training matrix $X$ and targets $\\mathbf{y}$. Use the object's **[fit](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression.fit)** method.\n",
    "\n",
    "The variable holding a reference to your _LinearRegression_ object should be called `linear_model`, so that your answer can be checked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 2 lines.\n",
    "linear_model = sklearn.linear_model.LinearRegression(fit_intercept=False)\n",
    "linear_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your answer** by running the code cell below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'linear_model' in globals(), \"You didn't create a variable named 'linear_model'!\"\n",
    "assert isinstance(linear_model, sklearn.linear_model.LinearRegression), \"Expected a LinearRegression instance!\"\n",
    "assert hasattr(linear_model, 'coef_'), \"No model coefficients yet! You didn't fit the model to any data!\"\n",
    "assert linear_model.intercept_ == 0.0, \"You forgot to disable fitting of the intercept!\"\n",
    "assert np.allclose(linear_model.coef_, [[1.57104472, 0.92521608]]), \"The model parameters you learned seem incorrect!\"\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot several LinearRegression model predictions at once** by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.ones(20)                        # A column of 1s so that the bias term w[0] gets added\n",
    "x1 = np.linspace(-5, 5, 20)             # A column of x values ranging from [-5, 5]\n",
    "X_test = np.column_stack([x0, x1])      # A 20x2 matrix where X[i,:] is the ith x vector\n",
    "y_test = linear_model.predict(X_test)   # Evaluate all x values\n",
    "plt.scatter(x1, y_test, 10, 'r')\n",
    "plt.scatter(X[:,1], y, edgecolor='b', facecolor='none')\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.title(\"Sample predictions for LinearRegression model\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also compare the model's `coef_` attribute (coefficients, i.e. model parameters) to the parameter vector $\\mathbf{w}$ that your own implementation gave from Exercise 2.4 (just use `print(w)` in your previous answer to see those values)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black; margin-bottom:5px\"></div>\n",
    "<div style=\"border-bottom: 3px solid black\"></div>\n",
    "\n",
    "## 3. Logistic regression\n",
    "\n",
    "Exercises 3.1&ndash;3.4 ask you to implement logistic regression, and to compare your results to applying the scikit-learn **[LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)** model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 3.1 &mdash; Vectorized code for generating predictions from a logistic model \n",
    "\n",
    "Recall from lecture that the logistic model has the form:\n",
    "$$\n",
    "\\hat{y}(\\mathbf{x}, \\mathbf{w}) = \\sigma(\\mathbf{x}^T \\mathbf{w})\n",
    "$$\n",
    "\n",
    "where $\\mathbf{x}$ and $\\mathbf{w}$ are the same as for Exercise 2.1 and $\\sigma(\\cdot)$ is the logistic sigmoid function described in Lecture 1.\n",
    "\n",
    "**Write a function** that evaluates the logistic model in vectorized fashion, just like you did for Exercise 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Returns the element-wise logistic sigmoid of z.\"\"\"\n",
    "    # Your code here. Aim for 1 line.\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "def logistic_model_predict(X, w):\n",
    "    \"\"\"\n",
    "    Returns predictions from logistic model y(x, w) at each point X[i,:] using parameters w.\n",
    "    Given X with shape (N,D+1), w must have shape (D+1,) and the result will have shape (N,).\n",
    "    \"\"\"\n",
    "    # Your code here. Aim for 1-2 lines.\n",
    "    return sigmoid(X @ w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your answer** by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = sigmoid(np.array([-1., 0., 1.5]))\n",
    "assert isinstance(y, np.ndarray), \"Expected an ndarray!\"\n",
    "assert np.allclose(y, [0.26894142, 0.5, 0.81757448]), \"Values from sigmoid() appear to be wrong!\"\n",
    "w = np.array([2, 1.5])                           # Parameters corresponding to logistic model y = sigmoid(2 + 1.5*x1)\n",
    "X = np.array([[1., -2.], [1.,  0.], [1.,  2.]])  # Evaluate at x1 = -2, 0, 2\n",
    "y = logistic_model_predict(X, w)                 # Predict y for all X using w\n",
    "assert isinstance(y, np.ndarray), \"Expected an ndarray!\"\n",
    "assert np.allclose(y, [0.26894142, 0.88079708, 0.99330715]), \"Wrong returned!\\n%s\" % y\n",
    "try:\n",
    "    y = logistic_model_predict(X, w.reshape(-1, 1))\n",
    "except ValueError:\n",
    "    raise AssertionError(\"Your answer works when 'w' is 1-dimensional, but not when it is a column vector. Try again.\")\n",
    "w = np.array([1, 0.5, 0.25])                      # Parameters corresponding to the 2D plane y = 1 + 0.5*x1 + 0.25*x2\n",
    "X = np.array([[1., -3., 1.], [1.,  3., 0.], [1.,  5., -2.]])   # Evaluate at different (x1, x2) points\n",
    "y = logistic_model_predict(X, w)                  # Predict y for all X using w\n",
    "assert np.allclose(y, [0.4378235, 0.92414182, 0.95257413]), \"Wrong predictions for 2-dimensional feature space!\\n%s\" % y\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot several predictions at once** by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([2, 1.5])                       # Parameters corresponding to logistic model y = sigmoid(2 + 1.5*x1)\n",
    "x0 = np.ones(20)                             # A column of 1s so that the bias term w[0] gets added\n",
    "x1 = np.linspace(-5, 5, 20)                  # A column of x values ranging from [-5, 5]\n",
    "X_test = np.column_stack([x0, x1])           # A 20x2 matrix where X[i,:] is the ith x vector\n",
    "y_test = logistic_model_predict(X_test, w)   # Evaluate all x values\n",
    "plt.scatter(x1, y_test, 10, 'r')\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.title(\"Sample predictions for logistic model $y=\\sigma(2 + \\\\frac{3}{2}x_1)$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 3.2 &mdash; Logistic regression by gradient descent\n",
    "\n",
    "Recall from Lecture 1 that the basic logistic regression training objective (learning objective) is:\n",
    "\n",
    "$$\n",
    "\\ell_\\text{LR}(\\mathbf{w}) = \\sum_{i=1}^N y_i \\ln \\sigma(\\mathbf{w}^T \\mathbf{x}_i) + (1-y_i) \\ln \\left(1-\\sigma(\\mathbf{w}^T \\mathbf{x}_i)\\right)\n",
    "$$\n",
    "\n",
    "The \"basic\" gradient for the above training objective is on a slide titled \"Maximum likelihood estimate for LR\" from Lecture 1, and reproduced here:\n",
    "\n",
    "$$\n",
    "\\nabla \\ell_\\text{LR}(\\mathbf{w}) = \\sum_{i=1}^N (\\sigma(\\mathbf{w}^T \\mathbf{x}_i) - y_i)\\mathbf{x}_i\n",
    "$$\n",
    "\n",
    "**Write a function** to implement logistic regression by gradient descent. Your answer to _logistic_regression_grad_ should ideally be fully vectorized (no for-loops), but this may take a while to figure out. If you can't figure out the vectorization, it's OK &mdash; just compute the gradient however you can. Your answer to _logistic_regression_ should use your _logistic_regression_grad_ function to compute the gradient at each step.\n",
    "\n",
    "Implementing _logistic_regression_grad_ is the hardest exercise in this lab because a vectorized implementation requires using the `@` matrix multiply operator to compute all the $\\mathbf{w}^T \\mathbf{x}$ products, reshaping the vector of residuals into a column-vector to use Numpy's broadcasting feature, and then summing over a specific axis (over training cases $i=1,\\ldots,N$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_grad(X, y, w):\n",
    "    \"\"\"Returns the gradient for basic logistic regression.\"\"\"\n",
    "    # Your code here. Aim for 1-3 lines.\n",
    "    r = sigmoid(X @ w) - y  # compute residual errors\n",
    "    return r @ X            # scale each x_i by its residual, and add them up\n",
    "\n",
    "def logistic_regression(X, y, w_init, learn_rate=0.05, num_steps=500):\n",
    "    \"\"\"\n",
    "    Fits a logistic model by gradient descent.\n",
    "    \n",
    "    If the feature matrix X has shape (N,D) the targets y should have shape (N,)\n",
    "    and the initial parameters w_init should have shape (D,).\n",
    "    \n",
    "    Returns a new parameter vector w that minimizes the negative log likelihood of the targets.\n",
    "    \"\"\"\n",
    "    # Your code here. Aim for 4-5 lines.\n",
    "    w = w_init\n",
    "    for i in range(num_steps):\n",
    "        grad = logistic_regression_grad(X, y, w)\n",
    "        w = w - learn_rate*grad\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your answer** by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[1, -1.0], [1, 1.0], [1, 2.0]])\n",
    "y = np.array([0.0, 0.0, 1.0])\n",
    "grad = logistic_regression_grad(X, y, np.array([0.0, 1.0]))\n",
    "assert isinstance(grad, np.ndarray), \"Expected ndarray from logistic_regression_grad!\"\n",
    "assert grad.shape == (2,), \"Expected gradient to have shape (2,) but was %s\" % (grad.shape,)\n",
    "assert np.allclose(grad, [0.88079708, 0.22371131]), \"Wrong value for gradient!\"\n",
    "grad = logistic_regression_grad(X, y, np.array([-1.0, 1.5]))\n",
    "assert np.allclose(grad, [0.57911459, 0.30819531]), \"Wrong value for gradient!\"\n",
    "w = logistic_regression(X, y, np.array([1.0, 0.0]))\n",
    "assert isinstance(w, np.ndarray), \"Expected ndarray from logistic_regression!\"\n",
    "assert w.shape == (2,), \"Expected parameter vector w to have shape (2,) but was %s\" % (w.shape,)\n",
    "assert np.allclose(w, [-4.14100532, 2.95489589]), \"Parameters found by gradient descent seem wrong!\"\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3 &mdash; Run logistic regression on data and plot the result\n",
    "\n",
    "For this exercise you'll need to define Numpy arrays that correspond to the following training data:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "1 & -4.1\\\\\n",
    "1 & -2.8\\\\\n",
    "1 & -0.7\\\\\n",
    "1 &  3.5\n",
    "\\end{bmatrix}, \\quad\n",
    "\\mathbf{y} = \\begin{bmatrix}\n",
    "0 \\\\\n",
    "0\\\\\n",
    "1\\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Write code** to create the following plot:\n",
    "![image](img/fig-logistic-regression-1d-train-and-test.png)\n",
    "\n",
    "Your code should follow this sequence of steps, which are the same as for Exercise 2.4:\n",
    "1. Make ndarrays $X$ and $\\mathbf{y}$ that contain the above training set.\n",
    "2. Plot the training set in blue. Use the $x$ coordinates from the second column of $X$, ignoring the first column.\n",
    "3. Run logistic regression on $(X, \\mathbf{y})$ to get fitted parameters $\\mathbf{w}$; use your *logistic_regression* function, starting from $\\mathbf{w}_\\text{init} = \\begin{bmatrix} 0.0, 1.0 \\end{bmatrix}^T$\n",
    "4. Define a \"test set\" of 20 equally-spaced values of $x$ in range $[-5, 5]$. You will need to build a new matrix $X_\\text{test}$ with $1$ in the first column and the 20 distinct $x$ values in the second column.\n",
    "5. Predict 20 $y$ values corresponding to the 20 rows of $X_\\text{test}$ by applying a logistic model with your fitted parameters $\\mathbf{w}$. Do this with single call to your *logistic_model_predict* function. \n",
    "6. Plot the predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the training set. Aim for 2 lines.\n",
    "X = np.array([[1, -4.1],\n",
    "              [1, -2.8],\n",
    "              [1, -0.7],\n",
    "              [1,  3.5]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "\n",
    "# 2. Plot the training set. Aim for 1 line.\n",
    "plt.scatter(X[:,1], y, edgecolors='b', facecolors='none', label='training points')\n",
    "\n",
    "# 3. Run logistic regression on the training set to get 'w'. Aim for 1-2 lines.\n",
    "w_init = np.array([0.0, 1.0])\n",
    "w = logistic_regression(X, y, w_init)\n",
    "\n",
    "# 4. Define the test set matrix of shape (20,2). Aim for 1-3 lines.\n",
    "X_test = np.column_stack([np.ones(20), np.linspace(-5, 5, 20)])\n",
    "\n",
    "# 5. Use the linear model to make predictions on the test set. Aim for 1 line.\n",
    "y_test = logistic_model_predict(X_test, w)\n",
    "\n",
    "# 6. Plot the test predictions. Aim for 1 line, plus a few lines to configure the plot (axis labels etc).\n",
    "plt.scatter(x1, y_test, 10, 'r', label='test predictions')\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.title(\"logistic regression\")\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border-bottom: 3px solid black;\"></div>\n",
    "\n",
    "### Exercise 3.4 &mdash; Run scikit-learn LogisticRegression\n",
    "\n",
    "The scikit-learn package provides a **[LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)** object to perform logistic regression.\n",
    "\n",
    "**Write code to fit a LogisticRegression model** using the same training matrix $X$ that you defined as part of Exercise 3.3. There are only two steps:\n",
    "1. Create the _LogisticRegression_ object. Do not fit an \"intercept\" and do not include any regularization penalty (see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)).\n",
    "2. Fit the _LogisticRegression_ object to the training matrix $X$ and targets $\\mathbf{y}$. Use the object's **[fit](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit)** method.\n",
    "\n",
    "The variable holding a reference to your _LogisticRegression_ object should be called `logistic_model`, so that your answer can be checked.\n",
    "\n",
    "A tweet regarding the fact that scikit-learn's LogisticRegression object applies regularization (a weight penalty) \"by default\":\n",
    "![image](img/fig-logistic-regression-regularization-tweet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. Aim for 2 lines.\n",
    "logistic_model = sklearn.linear_model.LogisticRegression(fit_intercept=False, penalty='none')\n",
    "logistic_model.fit(X, y);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check your answer** by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'logistic_model' in globals(), \"You didn't create a variable named 'logistic_model'!\"\n",
    "assert isinstance(logistic_model, sklearn.linear_model.LogisticRegression), \"Expected a LogisticRegression instance!\"\n",
    "assert hasattr(logistic_model, 'coef_'), \"No model coefficients yet! You didn't fit the model to any data!\"\n",
    "assert logistic_model.intercept_ == 0.0, \"You forgot to disable fitting of the intercept!\"\n",
    "assert np.allclose(logistic_model.coef_, [[18.5251137, 10.49283446]]), \"The parameters seem incorrect! Not L-BFGS? Used penalty?\"\n",
    "print(\"Correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the model parameters (coefficients) found by the _LogisticRegression_ are much larger than those found by your gradient descent solver. That is only because scikit-learn uses a more powerful optimization algorithm and can learn very sharp decision boundaries in fewer steps than mere gradient descent can. If you increase your *num_steps* argument your solver will find similarly large coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plot several LogisticRegression predictions at once** by running the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = np.ones(50)                               # A column of 1s so that the bias term w[0] gets added\n",
    "x1 = np.linspace(-5, 5, 50)                    # A column of x values ranging from [-5, 5]\n",
    "X_test = np.column_stack([x0, x1])             # A 20x2 matrix where X[i,:] is the ith x vector\n",
    "y_test = logistic_model.predict_proba(X_test)  # Evaluate all x values and get two probabilities back (class 0, class 1)\n",
    "plt.scatter(x1, y_test[:,1], 10, 'r')          # Plot probability of class 1 only\n",
    "plt.scatter(X[:,1], y, edgecolor='b', facecolor='none')\n",
    "plt.xlabel(\"$x_1$\")\n",
    "plt.ylabel(\"$y$\")\n",
    "plt.title(\"Sample predictions for LogisticRegression model\");"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
